---
title: "Dimensionality reduction and clustering II"
author: "Tiena Danner & Steven Moran"
date: "(`r format(Sys.time(), '%d %B, %Y')`)"
always_allow_html: true
output:
  github_document:
      toc: true
bibliography: '../2_writing_scientific_reports/references.bib'
---

***

This report uses the [R programming language](https://cran.r-project.org/doc/FAQ/R-FAQ.html) [@R] and the following [R libraries](https://r-pkgs.org/intro.html) [@tidyverse;@knitr;@ggpubr;@mlbench;@factoextra;@gridExtra;@cluster;@ggdendro].

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(ggpubr)
library(factoextra)
library(mlbench)
library(datasets)
library(gridExtra)
library(cluster)
library(ggdendro)
```


## PCA -- so what?

Recall the materials on [PCA](https://github.com/bambooforest/IntroDataScience/tree/main/10_Dimensionality_reduction_clustering_I). As you remember, we may use **PCA** to boil down sets of multivariate data into a few uncorrelated variables that capture the main axes of variation in a multivariate data set (the so-called principle components or PCs for short). We can use the resulting PCs to visualize multivariate data in low-dimensional space and we may even make inferences about potential patterns of variation within the data. 

**But what can we do to find the actual patterns, e.g., clusters in the data?**

In this chapter we will focus on "**How to find actual patterns in multivariate data**". There is a wide range of clustering methods in the programming world and we will only cover a tony fraction of them here. For an overview, see the Wikipedia article on [**clustering**](https://en.wikipedia.org/wiki/Cluster_analysis). The most widely used clustering technique is called [**K-means clustering**](https://en.wikipedia.org/wiki/K-means_clustering) and we will take a closer look at it today. 

We will not go into the mathematical details of clustering algorithms, but instead focus on their practical applications. The main purpose of clustering can be summarized as follows ([source](https://en.wikipedia.org/wiki/Cluster_analysis)):

**Clustering may be used for grouping objects (or subjects, specimens, etc.), such that objects in the same group (or cluster) are more similar to each other than objects in other clusters.**

The basic idea of this chapter is 1) that you can implement a clustering procedure on your own in R, 2) that you understand which steps are included in a classical clustering analysis and 3) finally to apply the procedure on your own on different data sets -- and of course your own data! 

Now let's go through a **cluster analysis** in detail. 


## Data

For this chapter we will utilize different data sets. First we will use the `iris` data from the `datasets` package. For more info on the iris data, consult this [webpage](http://archive.ics.uci.edu/ml/datasets/Iris). Later we'll use again the [Howells Data](https://web.utk.edu/~auerbach/HOWL.htm) [@Howells1973;@Howells1989;@Howells1995]. However, we will make use of an aggregated version of the Howells data which consists of only two entries per population (one female and male "mean" per population). 

Let's load the data (Iris and then the Howells data).

```{r, message=FALSE}
data(iris)
```

```{r, message=FALSE}
howells_mean <- read.csv("data/howells_mean.csv", header = TRUE, fill = TRUE, dec = ".")
```


**The first step** is always to take a closer look at the data that you are working with.

```{r}
head(iris) %>% kable()
head(howells_mean) %>% kable()
```


## K-means clustering in R

If you want to know in detail how clustering algorithms (specifically the K-means algorithm) works, you can consult these great resources that show the complete implementation in R: 

* [Data flair training](https://data-flair.training/blogs/clustering-in-r-tutorial/)
* [Geeks for geeks](https://www.geeksforgeeks.org/clustering-in-r-programming/)

To get an overview of how the method theoretically works, watch [this video](figures/StatQuest_K-means_clustering.mp4) (**attention**: if you click this link you will forwarded to a GitHub page which will say: *(Sorry about that, but we can't show files that are this big right now.)*, just click *View Raw* and you will be able to watch the video directly on GitHub). If you want the YouTube URL, [here](https://www.youtube.com/watch?v=4b5d3muPQmA) you go. 

In **summary**, the K-means algorithm works as follows ([source](https://www.geeksforgeeks.org/clustering-in-r-programming/)): 

* First, the **number of clusters** (groups) must be specified. The simplest case is two clusters. 
* Second, each data point is **randomly assigned** to one of the two clusters. 
* Third, the **centroids** of each data cluster are computed. The centroid is the "midpoint" of each of the cluster's data scatter.
* Then, the data points are **re-allocated to their closest centroid**, e.g., points are added to other clusters if they happen to be closer to another centroid (calculated via sum of squared distances).
* Then the new centroids are re-calculated.

Steps 3-4 are **repeated (aka iterated) until a global optimum has been reached**, i.e., no points can be re-allocated to other clusters.


### K-means clustering in R-code 

First, let's work with the `iris` data. 

```{r}
## We are only keeping numerical variables, so we get rid of the species name
iris_mod <- iris[, 1:4]

## Now we scale the data, this has to be done, especially when you've got measurements of different scales in your data (e.g., temperatures, wind speeds and UV-index)
iris_mod <- scale(iris_mod)

## Compute the K-means clustering algorithm
km <- kmeans(iris_mod, centers = 3, nstart = 25)

## Visualize the clusters
fviz_cluster(km, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE))
```

OK -- so what happened in the code above? 

* First we used indexing in R to remove the species name of the flower from the data with the command: `iris_mod <- iris[,1:4]`.
* Then we scaled the numerical variables to mean = 0 and standard deviation = 1 with the command: `iris_mod <- scale(iris_mod)`.
* Next we computed the K-means algorithm with the code snippet: `km <- kmeans(iris_mod, centers = 3, nstart = 25)`. We used 3 clusters for a start and the number of random sets = 25 (i.e., a random set of points for initializing the algorithm).
* The plot we produce with the command: `fviz_cluster(km, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE))`, which shows a PCA plot of the first two PCs and indicates the clusters that the algorithm found with different colors. 

What if we want to compare different numbers of clusters? No problem. Here we go.

```{r}
## Compute different sets of kmeans clustering with differnt number of clusters
kmeans2 <- kmeans(iris_mod, centers = 2, nstart = 25)
kmeans3 <- kmeans(iris_mod, centers = 3, nstart = 25)
kmeans4 <- kmeans(iris_mod, centers = 4, nstart = 25)
kmeans5 <- kmeans(iris_mod, centers = 5, nstart = 25)

# Comparing the Plots
plot1 <- fviz_cluster(kmeans2, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE)) + ggtitle("k = 2")
plot2 <- fviz_cluster(kmeans3, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE)) + ggtitle("k = 3")
plot3 <- fviz_cluster(kmeans4, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE)) + ggtitle("k = 4")
plot4 <- fviz_cluster(kmeans5, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE)) + ggtitle("k = 5")
grid.arrange(plot1, plot2, plot3, plot4, nrow = 2)
```

### How many clusters to choose in the end? 

This is a good but difficult question to answer. Generally it is hard to determine the exact number of clusters definitely. In the `iris` example, it makes sense to look at the original data. If you take a look at the number of flower species in the data, we find that there are **three species**. Let's plot a PCA and look at the variation between species.

```{r}
## compute the PCA -- you know how to do this
iris_pca <- prcomp(iris[, 1:4], scale. = TRUE)
iris_pca_dat <- cbind(iris, iris_pca$x[, 1:4])

## plot the first two PCs
ggplot(iris_pca_dat, aes(x = PC1, y = PC2, color = Species)) +
  geom_point() +
  stat_ellipse() +
  coord_equal() +
  theme_pubr(border = TRUE, margin = TRUE)
```

It becomes clear that here we would probably use three clusters, since it would then cluster roughly each species in a cluster (which makes sense in terms of their biology).

**But what if we do not have any prior knowledge about potential clusters?** This is normally case when clustering...

For example, this may often be the case when you have a number of study objects which you cannot group *a priori* into known clusters or groups. **That is where clustering really becomes interesting and useful!** There are certain [methods](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/) to find out how many clusters one can utilize for an analysis. We will show you three methods here, but without going into the mathematical details behind it. However, beware because these methods give **statistical cues** rather than absolute truths! 

There are three main methods to find out how many clusters make sense to plot ([source](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/)): 

* The Elbow method 
* The Silhouette method 
* The Gap statistic method

The **Elbow method** looks at the total WSS (within-cluster sums of squares) as a function of the number of clusters. It chooses the number of clusters such that adding another cluster does not improve the total WSS. This can be done by looking at the Elbow plot.

The **Silhoutte method** measures the quality of a clustering. Meaning, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.

The **Gap statistic method** compares the total intra-cluster variation for different values of K (number of clusters) with their expected values under a null reference distribution of the data. The estimate of the optimal clusters will be a value that maximizes the gap statistic (i.e, that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

Here are some R implementations of the three methods: 

```{r}
# Elbow method
fviz_nbclust(iris_mod, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")

# Silhouette method
fviz_nbclust(iris_mod, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")

# Gap statistic
gap_stat <- clusGap(iris_mod,
  FUN = kmeans, nstart = 25,
  K.max = 10, B = 10
)
print(gap_stat, method = "firstmax")
```
We can interpret the results as follows: 

* In the **elbow plot**, cluster number 4 does not really improve the total within sum of squares any more. 
* The **Silhouette** method yields an optimal silhouette width for 2 clusters. 
* The output of the **gap statistic** yields the largest gap statistic for 3 clusters `(--> Number of clusters (method 'firstmax'): 3)`

Given these results, we should probably choose **2-3 clusters**, which makes sense since there are three species in the `iris` data!


## Hierarchical clustering in R

Lastly, we will show **hierarchical clustering**. Hierarchical clustering is a bit different from K-means clustering. It essence, it does the following ([source](https://towardsdatascience.com/hierarchical-clustering-and-its-applications-41c1ad4441a6)): 

>>> Hierarchical clustering is a powerful technique which allows you to build tree structures (quite similar to phylogenetic trees) from data similarities. With hierarchical clustering you will be able to see how different subclusters relate to each other and how far data points are spaced form each other.

For showing how hierarchical clustering works, we will use the Howells data again. **Let's find out if certain populations relate more to each other in terms of craniometric measurements!** (See also [craniometry](https://en.wikipedia.org/wiki/Craniometry).)

```{r}
## Give meaningful rownames
rownames(howells_mean) <- c(paste(howells_mean[, 3], howells_mean[, 2]))

## Select only numerical variables and only females
howells_cluster <- howells_mean[which(howells_mean$SEX == "F"), 7:53]

## Clustering
d <- dist(howells_cluster, method = "euclidean") # distance matrix computation e.g., distance between individual points
fit <- hclust(d, method = "ward.D")

## Plot the dendrogram (aka tree)
ggdendrogram(fit)
```

Note that we only look at **females**, since between-sex variation would probably obscure similarities / dissimilarities between populations. Here we used the method **ward.D** in the `fit <- hclust(d, method = "ward.D")` command, which is a specific method to calculate the clustering. Feel free to try out different methods by changing the `method = "ward.D"`. You can find more info on the different methods [here](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html). 


## Clustering in essence

* Clustering is a statistical procedure that finds grouping patterns within a set of data by finding similarities between data points in the data.
* Clustering will help you to find potential distinct groups or clusters within a set of data. That is, clustering algorithms will create groups where data entries in a similar group will potentially have similar characteristics to each other.

**BUT** (there is always a but!): Keep in mind that clustering algorithms are statistical procedures and that the resulting patterns do not necessarily correspond to biologically (or other scientifically) relevant clustering patterns. Therefore, **always be careful how you interpret your clustering analysis** and be cautious of faulty conclusions or arbitrary clusters. Clustering is merely an exploratory tool and should not be used to draw ultimate conclusions about your analyses!

Here are some points and pitfalls to consider when doing clustering ([source](https://towardsdatascience.com/common-mistakes-in-cluster-analysis-and-how-to-avoid-them-eb960116d773)): 

- Make sure not to skip the step of **exploratory data analysis** (look at your data!) and do data cleaning if needed. Then consider:
  - What effects do cleaning steps have on the outcome of the clustering? 
  - What impacts do the outliers have on the clustering? 
- Use **scaled input variables**!
  - Never forget to scale your data input variables befor the clustering procedure.
  - Especially if your input variables have different measurement units!
  - If you forget this step, it will mess with the outcome of the clustering.
- Be careful of **arbitrary clustering**!
  - Make sure to choose a reasonable number of clusters that make scientific sense.
  - Use methods to determine the optimal number of clusters (e.g., see [section](#how-many-clusters-to-choose)).
  - It is a good rule of thumb to have about similar number of data points in each cluster (but this depends on your original data structure).
- Make sure to **describe the cluster patterns comprehensibly**!
  - Which characteristics represent each of the clusters?
  - Which characteristics distinguish your clusters?
  
# Data practical

* As always, write a nicely structured **scientific report** in R Markdown, which you will eventually upload on GitHub.
* Look at the data sets that are available from the `datasets` package or from the `mlbench` package. For example the `iris` data from the `datasets` package or the BreastCancer data from the `mlbench` package. For an overview of the different data sets that are available, you can consult this [webpage](https://machinelearningmastery.com/machine-learning-datasets-in-r/). 
* Here are a few examples, that are nicely fit to do a clustering analysis:

```{r}
## Iris data set
data(iris)

## BreastCancer data set
data(BreastCancer)

## US arrest reason data of different regions
data(USArrests)

## Car data of different car brands
data(mtcars)
```

* When you run the command `data("datasetname")`, the data will load in your global environment in R-Studio.
* Load some data (sets) from the packages above and run a cluster analysis (you can of course also use your own data!).
* Formulate 1-2 questions that you want to answer by doing a clustering analysis.
* Visualize your cluster analysis as shown in the lectures.
* Make sure to pick a reasonable number of clusters by applying appropriate methods (you received the tools in the lecture).
* Clearly communicate and explain the patterns you can see from your cluster analysis.
* Draw potential conclusions from your data analysis and interpret your results.
* Produce some visually appealing plots and statistics.
* Have fun!


# References
