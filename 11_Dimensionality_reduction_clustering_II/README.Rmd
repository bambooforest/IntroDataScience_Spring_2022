---
title: "Dimensionality reduction and clustering II"
author: "Tiena Danner"
date: "`r format(Sys.time(), '%d %B, %Y')`"
always_allow_html: true
output:
  github_document:
      toc: true
bibliography: '../2_writing_scientific_reports/references.bib'
---

***

This report uses the [R programming language](https://cran.r-project.org/doc/FAQ/R-FAQ.html) [@R] and the following [R libraries](https://r-pkgs.org/intro.html) [@tidyverse;@knitr;@ggpubr;@mlbench;@factoextra;@gridExtra].

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(ggpubr)
library(factoextra)
library(mlbench)
library(datasets)
library(gridExtra)
library(cluster)
```

## PCA -- so what?

Recall the materials on [PCA](https://github.com/bambooforest/IntroDataScience/tree/main/10_Dimensionality_reduction_clustering_I). As you remember, we may use **PCA** to boil down sets of multivariate data into a few uncorrelated variables that explain the main axes of variation in our data (the PCs). We can use the PCs to look at our data and how it is structured and even find some clusters within the data. 

**But: what can we do to find the actual patterns e.g. clusters in the data?**

In this chapter we will focus on "**how to patterns in multivariate data --> clustering**". There is a wide range of clustering methods in the programming world and we will only cover a fraction of those. For an overview, see the Wikipedia article on [**clustering**](https://en.wikipedia.org/wiki/Cluster_analysis). The most widely used technique for clustering is [**K-means clustering**](https://en.wikipedia.org/wiki/K-means_clustering), which we will apply today. 

Again, we will not go into the exact mathematical details of clustering algorithms but focus on the practical application of clustering. The main goals of clustering can be summarized as [follows](https://en.wikipedia.org/wiki/Cluster_analysis):

**Grouping objects (or subjects/specimens) such that objects in the same group (cluster) are more similar to each other than objects in other clusters**

The basic idea of this chapter is that you can implement a clustering procedure on your own, that you understand which steps are included in a classical clustering analysis and finally to apply it on a different data set (potentially your own data). 

Now let's go through a **cluster analysis** in detail. 

## Data

Here we will use different kind of data to visualize the clustering optimally. First we will use the `iris` data from the `datasets` package. For more info on the iris data, consider this [webpage](http://archive.ics.uci.edu/ml/datasets/Iris). Later we'll use again the [Howells Data](https://web.utk.edu/~auerbach/HOWL.htm) [@Howells1973;@Howells1989;@Howells1995]. However, we will have two versions of data for the Howells Data, the full data set and an aggregated data set of only two entries per population (one female and male "mean"). 

Let's load the data. 

```{r, message=FALSE}
howells <- read_csv("data/howells_data.csv")
howells_mean <- read_csv("data/howells_mean.csv")
data(iris)
head(iris) %>% kable()
```

**First step always**, take a close look at the data. We will skip this step for now since you already know the data.

## K-means clustering in R

If you want to know in detail how clustering algorithms (specifically the K-means algorithm) works, you can consider the following resources that show the complete implementation in R: 

* [data flair training](https://data-flair.training/blogs/clustering-in-r-tutorial/)
* [geeks for geeks](https://www.geeksforgeeks.org/clustering-in-r-programming/)

To get an overview of how it works, watch [this video](figures/StatQuest_K-means_clustering.mp4) (**attention**: if you click this link you will forwarded to a github page which will say: *(Sorry about that, but we canâ€™t show files that are this big right now.)*, just click *View Raw* and you will be able to watch the video directly on github). If you want the Youtube link, [here](https://www.youtube.com/watch?v=4b5d3muPQmA) you go. 

In summary, the K-means algorithm works as follows ([source](https://www.geeksforgeeks.org/clustering-in-r-programming/)): 

* First, the **number of clusters** (groups) must be specified. The simplest case would be 2 clusters. 
* Second, each data point is **randomly assigned** to one of the two clusters. 
* Third, the **centroids** of each data cluster are computed. The centroid is the "midpoint" of each of the cluster's data scatter.
* Then, the data points are **re-allocated to their closest centroid** e.g. points are added to other clusters, if they are nearer to another centroid.
* Then the new centroids are re-calculated 

Steps 3-4 are **repeated until a global optimum has been reached** e.g. no points can be re-allocated to other clusters.

### K-means clustering in R-code 

First, let's work on the `iris` data. 

```{r}
## we are only keeping numerical variables, we get rid of the species name 
iris_mod <- iris[,1:4] 

## now we scale the data, this has to be done especially when you've got measurements of different scales in your data (e.g. temperatures, windspeed and UV-index for example)
iris_mod <- scale(iris_mod)

## compute the K-means clustering algorithm 
km <- kmeans(iris_mod, centers = 3, nstart = 25)
 
## Visualize the clusters
fviz_cluster(km, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE))
```

OK -- what happened? 

* First we used indexing, to remove the species name of the flower from the data with the command `iris_mod <- iris[,1:4] `
* Then we scaled the numerical variables to mean = 0 and sd = 1 with the command `iris_mod <- scale(iris_mod)`
* Afterwards we could compute the K-means algorithm with the code `km <- kmeans(iris_mod, centers = 3, nstart = 25)`. We used 3 clusters for a start and the number of random sets = 25. 
* The plot we produce with `fviz_cluster(km, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE))` shows a PCA plot of the first two PCs and the clusters with different colors. 

What if we want to compare different numbers of clusters? No problem. Here we go. 

```{r}
kmeans2 <- kmeans(iris_mod, centers = 2, nstart = 25)  
kmeans3 <- kmeans(iris_mod, centers = 3, nstart = 25)  
kmeans4 <- kmeans(iris_mod, centers = 4, nstart = 25)  
kmeans5 <- kmeans(iris_mod, centers = 5, nstart = 25)  
#Comparing the Plots
plot1 <- fviz_cluster(kmeans2, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE)) + ggtitle("k = 2")
plot2 <- fviz_cluster(kmeans3, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE)) + ggtitle("k = 3")
plot3 <- fviz_cluster(kmeans4, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE)) + ggtitle("k = 4")
plot4 <- fviz_cluster(kmeans5, data = iris_mod, ggtheme = theme_pubr(border = TRUE, margin = TRUE)) + ggtitle("k = 5")
grid.arrange(plot1, plot2, plot3, plot4, nrow = 2)
```

### How many clusters to choose? 

This is a hard question and it is difficult to determine the number of clusters definitely. In the `iris` example it makes sense to look at the original data. If you take a look at the number of flower species in the data, we find that there are **three species**. Let's plot a PCA. 

```{r}
iris_pca <- prcomp(iris[,1:4], scale. = TRUE)
iris_pca_dat <- cbind(iris, iris_pca$x[, 1:4])

## plot the first two PCs
ggplot(iris_pca_dat, aes(x = PC1, y = PC2, color = Species)) +
  geom_point() +
  stat_ellipse() +
  coord_equal() +
  theme_pubr(border = TRUE, margin = TRUE)
```

It becomes clear that here, we would probably use three clusters, since it would then cluster roughly each species in a cluster. 

**But what if we do not have any prior knowledge about potential clusters?**

This will be often the case when you have a number of study objects which you cannot group per se into different clusters. That is where clustering really becomes interesting! There are certain [methods](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/) to find out, how many clusters one could utilize for an analysis. We will show you three methods here, without going into the mathematical details behind it. However, not that these methods rather give **statistical cues** rather than absolute truths! 

There are three main methods to find out how many clusters make sense to plot [source](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/): 

* The Elbow method 
* The Silhouette method 
* The Gap statistic method

The **Elbow method** looks at the total WSS (within-cluster sums of squares) as a function of the number of clusters: We should choose the number of clusters so, that adding another cluster does not improve much better the total WSS. This can be done by looking at the Elbow plot (see below). 

The **Silhoutte method** measures the quality of a clustering. Meaning, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.

The **Gap statistic method** compares the total intra-cluster variation for different values of K (number of clusters) with their expected values under a null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e, that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

Here are some R implementations of the three methods: 

```{r}
# Elbow method
fviz_nbclust(iris_mod, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")

# Silhouette method
fviz_nbclust(iris_mod, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")

# Gap statistic
gap_stat <- clusGap(iris_mod, FUN = kmeans, nstart = 25,
 K.max = 10, B = 10)
 print(gap_stat, method = "firstmax")
```

Given the results, we would probably choose **2-3 clusters**, which makes sense since there are three species in the data. 

Clustering in essence: 

***

* PCA is a statistical procedure that converts a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components ([Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis))
* PCA will help us to find a reduced number of features that will represent our original data set in a compressed way, capturing up to a certain portion of its variance depending on the number of new features we end up selecting ([Towardsdatascience](https://towardsdatascience.com/the-most-gentle-introduction-to-principal-component-analysis-9ffae371e93b))

***

**BUT** (there is always a but): Keep in mind that PCA (and other dimensionality reduction techniques) are statistical procedures and that the resulting PCs do not necessarily correspond to biologically relevant patterns of variation. Therefore, **always be careful how you interpret your PCA** and be cautious of faulty conclusions. PCA merely extracts patterns of variation in a sample and helps you to visualize  these patterns! 

## Hierarchical clustering in R


# Data practical

* As always, write a nicely structured **scientific report** in R Markdown, which you will eventually upload on github (you hopefully know how to do this until now :grin:)
* Look at the data sets that are available from the `datasets` package or from the `mlbench` package. For example the iris data from the `datasets` package or the BreastCancer data from the `mlbench` package. For an overview of the different data sets that are available, you can consult this [webpage](https://machinelearningmastery.com/machine-learning-datasets-in-r/). 
* Here are two examples, that are nicely fit to compute a PCA:

```{r}
## iris data set
data(iris)

## BreastCancer data set
data(BreastCancer)
```

* When you run the command `data("datasetname")`, the data will load in your global environment in R-Studio
* Load some data (sets) from the packages above and run a PCA (you can of course also use your own data!)
* Formulate two questions or hypotheses on the data, that you want to answer by using a PCA
* Visualize the PCA and utilize grouping procedures (coloring, density ellipses etc.) to find **patterns** in your data 
* Clearly communicate the explained variances with a screeplot 
* Investigate potential correlations of your PCs (you can also use a biplot for that)
* Produce some final plots and comment what you can see on them and how you interpret the results

# References
